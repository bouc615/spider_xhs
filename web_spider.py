#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦çˆ¬è™«Web API
æä¾›JSONæ•°æ®è¾“å‡ºå’ŒHTMLå¯è§†åŒ–äº¤äº’
"""

import json
import os
import time
import uuid
import requests
from datetime import datetime
from flask import Flask, request, jsonify, render_template, send_from_directory
from flask_cors import CORS
from threading import Thread
from main import Data_Spider
from xhs_utils.common_util import init
from loguru import logger

app = Flask(__name__)
CORS(app)


class WebSpider:
    def __init__(self):
        self.cookies_str, self.base_path = init()
        self.data_spider = Data_Spider()
        self.tasks = {}  # å­˜å‚¨ä»»åŠ¡çŠ¶æ€
        self.results_dir = "web_data"
        os.makedirs(self.results_dir, exist_ok=True)

    def extract_note_data(self, note_url, cookies_str=None):
        """
        æå–å•ä¸ªç¬”è®°çš„å®Œæ•´æ•°æ®
        :param note_url: ç¬”è®°URL
        :param cookies_str: Cookieå­—ç¬¦ä¸²ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„Cookie
        """
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„cookieï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤çš„
        cookies_to_use = cookies_str or self.cookies_str
        try:
            # è·å–ç¬”è®°åŸºæœ¬ä¿¡æ¯
            success, msg, note_info = self.data_spider.spider_note(
                note_url, cookies_to_use
            )
            if not success:
                logger.error(f"è·å–ç¬”è®°ä¿¡æ¯å¤±è´¥: {msg}")
                return None

            # è·å–è¯„è®º - ä½¿ç”¨ç®€åŒ–æ–¹æ³•
            logger.info(f"å¼€å§‹è·å–ç¬”è®°è¯„è®º: {note_url}")
            comments = []
            try:
                # è§£ænote_idå’Œxsec_token
                import urllib.parse

                urlParse = urllib.parse.urlparse(note_url)
                note_id = urlParse.path.split("/")[-1]

                xsec_token = ""
                if urlParse.query:
                    kvs = urlParse.query.split("&")
                    for kv in kvs:
                        if "=" in kv and kv.startswith("xsec_token="):
                            xsec_token = kv.split("=", 1)[1]
                            break

                logger.info(
                    f"è§£æå¾—åˆ° note_id: {note_id}, xsec_token: {xsec_token[:20]}..."
                )

                # è·å–ç¬¬ä¸€é¡µè¯„è®ºè¿›è¡Œæµ‹è¯•
                success, msg, res_json = self.data_spider.xhs_apis.get_note_out_comment(
                    note_id, "", xsec_token, cookies_to_use
                )

                if (
                    success
                    and res_json
                    and "data" in res_json
                    and "comments" in res_json["data"]
                ):
                    comments = res_json["data"]["comments"]
                    logger.info(f"æˆåŠŸè·å–è¯„è®ºæ•°é‡: {len(comments)}")
                else:
                    logger.warning(f"è·å–è¯„è®ºå¤±è´¥: {msg}")

            except Exception as e:
                logger.error(f"è·å–è¯„è®ºå¼‚å¸¸: {e}")
                comments = []

            # æå–å›¾ç‰‡é“¾æ¥
            pictures = []
            if "image_list" in note_info:
                for img in note_info["image_list"]:
                    img_url = None
                    if isinstance(img, dict) and "url" in img:
                        img_url = img["url"]
                    elif isinstance(img, str):
                        img_url = img

                    if img_url:
                        pictures.append(img_url)
                        logger.info(f"æå–åˆ°å›¾ç‰‡URL: {img_url}")

            # æå–è¯„è®ºå†…å®¹
            comment_texts = []
            logger.info(f"å¤„ç†è¯„è®ºæ•°é‡: {len(comments)}")

            for comment in comments:
                if isinstance(comment, dict):
                    # æå–ä¸»è¯„è®ºå†…å®¹
                    content = comment.get("content", "")
                    if content:
                        # æ¸…ç†è¯„è®ºå†…å®¹ï¼Œç§»é™¤è¡¨æƒ…ç¬¦å·æ ‡è®°
                        clean_content = content.replace("[å¤§ç¬‘R]", "ğŸ˜„").replace(
                            "[å·ç¬‘R]", "ğŸ˜"
                        )
                        comment_texts.append(clean_content)
                        logger.info(f"æå–è¯„è®º: {clean_content[:50]}...")

                    # æå–å­è¯„è®º
                    sub_comments = comment.get("sub_comments", [])
                    for sub_comment in sub_comments:
                        if isinstance(sub_comment, dict):
                            sub_content = sub_comment.get("content", "")
                            if sub_content:
                                clean_sub_content = sub_content.replace(
                                    "[å¤§ç¬‘R]", "ğŸ˜„"
                                ).replace("[å·ç¬‘R]", "ğŸ˜")
                                comment_texts.append(
                                    f"â†³ {clean_sub_content}"
                                )  # æ·»åŠ ç¼©è¿›æ ‡è¯†å­è¯„è®º
                                logger.info(f"æå–å­è¯„è®º: {clean_sub_content[:50]}...")
                elif isinstance(comment, str):
                    comment_texts.append(comment)

            logger.info(f"æå–åˆ°è¯„è®ºæ–‡æœ¬æ•°é‡: {len(comment_texts)}")

            return {
                "link": note_url,
                "title": note_info.get("title", ""),
                "content": note_info.get("desc", ""),
                "pictures": pictures,
                "comments": comment_texts,
            }

        except Exception as e:
            logger.error(f"æå–ç¬”è®°æ•°æ®å¤±è´¥: {e}")
            return None

    def search_and_collect(self, keyword, num_notes, task_id, cookie=None):
        """
        æœç´¢å¹¶æ”¶é›†æ•°æ®çš„åå°ä»»åŠ¡
        """
        try:
            logger.info(f"å¼€å§‹æœç´¢ä»»åŠ¡ {task_id}: {keyword}")
            self.tasks[task_id]["status"] = "running"
            self.tasks[task_id]["progress"] = 0

            # ä½¿ç”¨ä¼ å…¥çš„Cookieæˆ–é»˜è®¤Cookie
            cookies_str = cookie or self.cookies_str

            # æœç´¢ç¬”è®°
            success, msg, notes = self.data_spider.xhs_apis.search_some_note(
                keyword,
                num_notes,
                cookies_str,
                sort_type_choice=2,  # æŒ‰æœ€å¤šç‚¹èµæ’åº
                note_type=0,  # ä¸é™ç±»å‹
                proxies=None,
            )

            if not success:
                self.tasks[task_id]["status"] = "failed"
                self.tasks[task_id]["error"] = msg
                return

            # è¿‡æ»¤ç¬”è®°ç±»å‹
            notes = list(filter(lambda x: x["model_type"] == "note", notes))
            logger.info(f"æ‰¾åˆ° {len(notes)} æ¡ç›¸å…³ç¬”è®°")

            collected_data = []
            total_notes = min(len(notes), num_notes)

            for i, note in enumerate(notes[:num_notes]):
                try:
                    note_url = f"https://www.xiaohongshu.com/explore/{note['id']}?xsec_token={note['xsec_token']}"

                    logger.info(f"å¤„ç†ç¬¬ {i+1}/{total_notes} ä¸ªç¬”è®°...")
                    note_data = self.extract_note_data(note_url, cookies_str)

                    if note_data:
                        collected_data.append(note_data)

                    # æ›´æ–°è¿›åº¦
                    progress = int((i + 1) / total_notes * 100)
                    self.tasks[task_id]["progress"] = progress

                    # æ·»åŠ å»¶æ—¶é¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(1)

                except Exception as e:
                    logger.error(f"å¤„ç†ç¬¬ {i+1} ä¸ªç¬”è®°æ—¶å‡ºé”™: {e}")
                    continue

            # ä¿å­˜ç»“æœ
            result_data = {
                "task": keyword,
                "data": collected_data,
                "id": task_id,
                "created_at": datetime.now().isoformat(),
                "total_notes": len(collected_data),
            }

            result_file = os.path.join(self.results_dir, f"{task_id}.json")
            with open(result_file, "w", encoding="utf-8") as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)

            self.tasks[task_id]["status"] = "completed"
            self.tasks[task_id]["progress"] = 100
            self.tasks[task_id]["result_file"] = result_file

            logger.info(f"ä»»åŠ¡ {task_id} å®Œæˆï¼Œæ”¶é›†äº† {len(collected_data)} æ¡æ•°æ®")

        except Exception as e:
            logger.error(f"ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {e}")
            self.tasks[task_id]["status"] = "failed"
            self.tasks[task_id]["error"] = str(e)


web_spider = WebSpider()


@app.route("/")
def index():
    """ä¸»é¡µ"""
    return render_template("index.html")


@app.route("/api/search", methods=["POST"])
def start_search():
    """å¼€å§‹æœç´¢ä»»åŠ¡"""
    data = request.get_json()
    keyword = data.get("keyword", "").strip()
    num_notes = data.get("num_notes", 10)
    cookie = data.get("cookie", "").strip()

    if not keyword:
        return jsonify({"error": "å…³é”®è¯ä¸èƒ½ä¸ºç©º"}), 400

    if num_notes <= 0 or num_notes > 100:
        return jsonify({"error": "ç¬”è®°æ•°é‡å¿…é¡»åœ¨1-100ä¹‹é—´"}), 400

    if not cookie:
        return jsonify({"error": "ç™»å½•å‡­è¯ä¸èƒ½ä¸ºç©º"}), 400

    # ç”Ÿæˆä»»åŠ¡ID
    task_id = int(time.time() * 1000)  # ä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºID

    # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
    web_spider.tasks[task_id] = {
        "keyword": keyword,
        "num_notes": num_notes,
        "cookie": cookie,
        "status": "pending",
        "progress": 0,
        "created_at": datetime.now().isoformat(),
    }

    # å¯åŠ¨åå°ä»»åŠ¡
    thread = Thread(
        target=web_spider.search_and_collect, args=(keyword, num_notes, task_id, cookie)
    )
    thread.daemon = True
    thread.start()

    return jsonify(
        {"task_id": task_id, "message": "æœç´¢ä»»åŠ¡å·²å¯åŠ¨", "status": "pending"}
    )


@app.route("/api/test_cookie", methods=["POST"])
def test_cookie():
    """æµ‹è¯•ç™»å½•å‡­è¯æœ‰æ•ˆæ€§"""
    data = request.get_json()
    cookie = data.get("cookie", "").strip()

    if not cookie:
        return jsonify({"success": False, "message": "ç™»å½•å‡­è¯ä¸èƒ½ä¸ºç©º"}), 400

    # åŸºæœ¬æ ¼å¼éªŒè¯
    if "=" not in cookie or len(cookie) < 50:
        return (
            jsonify(
                {
                    "success": False,
                    "message": "ç™»å½•å‡­è¯æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·ç¡®ä¿å¤åˆ¶å®Œæ•´çš„èº«ä»½éªŒè¯ä¿¡æ¯",
                }
            ),
            400,
        )

    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„å­—æ®µ
    required_fields = ["a1", "web_session"]
    missing_fields = []
    for field in required_fields:
        if f"{field}=" not in cookie:
            missing_fields.append(field)

    if missing_fields:
        return (
            jsonify(
                {
                    "success": False,
                    "message": f"ç™»å½•å‡­è¯ç¼ºå°‘å¿…è¦å­—æ®µ: {', '.join(missing_fields)}ï¼Œè¯·ç¡®ä¿å·²ç™»å½•å°çº¢ä¹¦åå¤åˆ¶å®Œæ•´ä¿¡æ¯",
                }
            ),
            400,
        )

    try:
        # ä½¿ç”¨Cookieæµ‹è¯•ä¸€ä¸ªç®€å•çš„APIè°ƒç”¨
        success, msg, result = web_spider.data_spider.xhs_apis.get_homefeed_all_channel(
            cookie
        )

        if success:
            # å°è¯•è·å–ç”¨æˆ·ä¿¡æ¯æ¥è¿›ä¸€æ­¥éªŒè¯
            try:
                user_success, user_msg, user_info = (
                    web_spider.data_spider.xhs_apis.get_user_self_info(cookie)
                )
                if user_success and user_info and "data" in user_info:
                    user_name = user_info["data"].get("nickname", "æœªçŸ¥ç”¨æˆ·")
                    return jsonify(
                        {
                            "success": True,
                            "message": f"ç™»å½•å‡­è¯æœ‰æ•ˆï¼Œå½“å‰ç”¨æˆ·: {user_name}",
                        }
                    )
                else:
                    return jsonify({"success": True, "message": "ç™»å½•å‡­è¯æœ‰æ•ˆ"})
            except:
                return jsonify({"success": True, "message": "ç™»å½•å‡­è¯æœ‰æ•ˆ"})
        else:
            # æ ¹æ®é”™è¯¯ä¿¡æ¯æä¾›æ›´å…·ä½“çš„åé¦ˆ
            if "ç™»å½•" in msg or "login" in msg.lower():
                return jsonify(
                    {
                        "success": False,
                        "message": "ç™»å½•å‡­è¯å·²è¿‡æœŸï¼Œè¯·é‡æ–°ç™»å½•å°çº¢ä¹¦åè·å–æ–°çš„èº«ä»½éªŒè¯ä¿¡æ¯",
                    }
                )
            elif "æƒé™" in msg or "permission" in msg.lower():
                return jsonify(
                    {
                        "success": False,
                        "message": "ç™»å½•å‡­è¯æƒé™ä¸è¶³ï¼Œè¯·ç¡®ä¿å·²å®Œå…¨ç™»å½•å°çº¢ä¹¦",
                    }
                )
            else:
                return jsonify(
                    {"success": False, "message": f"ç™»å½•å‡­è¯éªŒè¯å¤±è´¥: {msg}"}
                )

    except Exception as e:
        logger.error(f"æµ‹è¯•ç™»å½•å‡­è¯å¤±è´¥: {e}")
        error_msg = str(e)
        if "timeout" in error_msg.lower():
            return jsonify(
                {"success": False, "message": "ç½‘ç»œè¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•"}
            )
        elif "connection" in error_msg.lower():
            return jsonify(
                {"success": False, "message": "ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®"}
            )
        else:
            return jsonify({"success": False, "message": f"æµ‹è¯•å¤±è´¥: {error_msg}"})


@app.route("/api/task/<int:task_id>/status")
def get_task_status(task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    if task_id not in web_spider.tasks:
        return jsonify({"error": "ä»»åŠ¡ä¸å­˜åœ¨"}), 404

    task = web_spider.tasks[task_id]
    return jsonify(
        {
            "task_id": task_id,
            "status": task["status"],
            "progress": task["progress"],
            "keyword": task["keyword"],
            "num_notes": task["num_notes"],
            "created_at": task["created_at"],
            "error": task.get("error", None),
        }
    )


@app.route("/api/data/<int:task_id>")
def get_data(task_id):
    """è·å–ä»»åŠ¡ç»“æœæ•°æ®"""
    result_file = os.path.join(web_spider.results_dir, f"{task_id}.json")

    if not os.path.exists(result_file):
        return jsonify({"error": "æ•°æ®ä¸å­˜åœ¨"}), 404

    try:
        with open(result_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        return jsonify(data)
    except Exception as e:
        return jsonify({"error": f"è¯»å–æ•°æ®å¤±è´¥: {str(e)}"}), 500


@app.route("/api/tasks")
def list_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨"""
    tasks_list = []
    for task_id, task_info in web_spider.tasks.items():
        tasks_list.append(
            {
                "id": task_id,
                "keyword": task_info["keyword"],
                "status": task_info["status"],
                "progress": task_info["progress"],
                "created_at": task_info["created_at"],
            }
        )

    # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
    tasks_list.sort(key=lambda x: x["created_at"], reverse=True)
    return jsonify(tasks_list)


@app.route("/view/<int:task_id>")
def view_result(task_id):
    """æŸ¥çœ‹ç»“æœé¡µé¢"""
    return render_template("result.html", task_id=task_id)


@app.route("/test_image")
def test_image():
    """å›¾ç‰‡ä»£ç†æµ‹è¯•é¡µé¢"""
    return render_template("test_image.html")


@app.route("/api/export/comments/<int:task_id>")
def export_comments(task_id):
    """å¯¼å‡ºè¯„è®ºæ•°æ®"""
    note_index = request.args.get("note_index", type=int)
    export_format = request.args.get("format", "json")  # json, csv, txt

    result_file = os.path.join(web_spider.results_dir, f"{task_id}.json")

    if not os.path.exists(result_file):
        return jsonify({"error": "æ•°æ®ä¸å­˜åœ¨"}), 404

    try:
        with open(result_file, "r", encoding="utf-8") as f:
            data = json.load(f)

        # æ”¶é›†è¯„è®ºæ•°æ®
        comments_data = []

        if note_index is not None:
            # å¯¼å‡ºç‰¹å®šç¬”è®°çš„è¯„è®º
            if 0 <= note_index < len(data["data"]):
                note = data["data"][note_index]
                for comment in note["comments"]:
                    comments_data.append(
                        {
                            "note_title": note["title"],
                            "note_link": note["link"],
                            "comment": comment,
                            "note_index": note_index,
                        }
                    )
        else:
            # å¯¼å‡ºæ‰€æœ‰è¯„è®º
            for idx, note in enumerate(data["data"]):
                for comment in note["comments"]:
                    comments_data.append(
                        {
                            "note_title": note["title"],
                            "note_link": note["link"],
                            "comment": comment,
                            "note_index": idx,
                        }
                    )

        # æ ¹æ®æ ¼å¼è¿”å›æ•°æ®
        if export_format == "json":
            response_data = {
                "task": data["task"],
                "export_time": datetime.now().isoformat(),
                "total_comments": len(comments_data),
                "comments": comments_data,
            }

            response = app.response_class(
                json.dumps(response_data, ensure_ascii=False, indent=2),
                mimetype="application/json",
                headers={
                    "Content-Disposition": f"attachment; filename=comments_{task_id}.json",
                    "Content-Type": "application/json; charset=utf-8",
                },
            )
            return response

        elif export_format == "csv":
            import csv
            import io

            output = io.StringIO()
            writer = csv.writer(output)

            # å†™å…¥è¡¨å¤´
            writer.writerow(["ç¬”è®°æ ‡é¢˜", "ç¬”è®°é“¾æ¥", "è¯„è®ºå†…å®¹", "ç¬”è®°ç´¢å¼•"])

            # å†™å…¥æ•°æ®
            for item in comments_data:
                writer.writerow(
                    [
                        item["note_title"],
                        item["note_link"],
                        item["comment"],
                        item["note_index"],
                    ]
                )

            response = app.response_class(
                output.getvalue(),
                mimetype="text/csv",
                headers={
                    "Content-Disposition": f"attachment; filename=comments_{task_id}.csv",
                    "Content-Type": "text/csv; charset=utf-8",
                },
            )
            return response

        elif export_format == "txt":
            output_lines = []
            output_lines.append(f"è¯„è®ºå¯¼å‡º - {data['task']}")
            output_lines.append(
                f"å¯¼å‡ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            )
            output_lines.append(f"è¯„è®ºæ€»æ•°: {len(comments_data)}")
            output_lines.append("=" * 50)
            output_lines.append("")

            current_note = None
            for item in comments_data:
                if current_note != item["note_title"]:
                    current_note = item["note_title"]
                    output_lines.append(f"ã€{current_note}ã€‘")
                    output_lines.append(f"é“¾æ¥: {item['note_link']}")
                    output_lines.append("-" * 30)

                output_lines.append(f"â€¢ {item['comment']}")
                output_lines.append("")

            response = app.response_class(
                "\n".join(output_lines),
                mimetype="text/plain",
                headers={
                    "Content-Disposition": f"attachment; filename=comments_{task_id}.txt",
                    "Content-Type": "text/plain; charset=utf-8",
                },
            )
            return response

        else:
            return jsonify({"error": "ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼"}), 400

    except Exception as e:
        logger.error(f"å¯¼å‡ºè¯„è®ºå¤±è´¥: {e}")
        return jsonify({"error": f"å¯¼å‡ºå¤±è´¥: {str(e)}"}), 500


@app.route("/proxy_image")
def proxy_image():
    """ä»£ç†å›¾ç‰‡è¯·æ±‚ï¼Œè§£å†³403é—®é¢˜"""
    image_url = request.args.get("url")
    if not image_url:
        logger.error("ä»£ç†å›¾ç‰‡è¯·æ±‚ç¼ºå°‘URLå‚æ•°")
        return "Missing URL parameter", 400

    try:
        logger.info(f"ä»£ç†å›¾ç‰‡è¯·æ±‚: {image_url}")

        # æ·»åŠ å°çº¢ä¹¦çš„è¯·æ±‚å¤´æ¥ç»•è¿‡é˜²ç›—é“¾
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Referer": "https://www.xiaohongshu.com/",
            "Accept": "image/webp,image/apng,image/*,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        }

        # æ·»åŠ è¶…æ—¶è®¾ç½®
        response = requests.get(image_url, headers=headers, timeout=10, stream=True)

        logger.info(f"å›¾ç‰‡è¯·æ±‚å“åº”çŠ¶æ€: {response.status_code}")

        if response.status_code == 200:
            # è·å–å†…å®¹ç±»å‹
            content_type = response.headers.get("content-type", "image/jpeg")

            # è¿”å›å›¾ç‰‡æ•°æ®
            return app.response_class(
                response.content,
                mimetype=content_type,
                headers={
                    "Cache-Control": "public, max-age=3600",  # ç¼“å­˜1å°æ—¶
                    "Access-Control-Allow-Origin": "*",
                },
            )
        else:
            logger.warning(f"å›¾ç‰‡è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return (
                f"Failed to fetch image: {response.status_code}",
                response.status_code,
            )

    except requests.exceptions.Timeout:
        logger.error(f"å›¾ç‰‡è¯·æ±‚è¶…æ—¶: {image_url}")
        return "Request timeout", 408
    except requests.exceptions.RequestException as e:
        logger.error(f"å›¾ç‰‡è¯·æ±‚å¼‚å¸¸: {e}")
        return f"Request error: {str(e)}", 500
    except Exception as e:
        logger.error(f"ä»£ç†å›¾ç‰‡è¯·æ±‚å¤±è´¥: {e}")
        return f"Error: {str(e)}", 500


if __name__ == "__main__":
    print("ğŸš€ å°çº¢ä¹¦çˆ¬è™«WebæœåŠ¡å¯åŠ¨...")
    print("ğŸ“± è®¿é—® http://localhost:8888 å¼€å§‹ä½¿ç”¨")
    app.run(debug=True, host="0.0.0.0", port=8888)
